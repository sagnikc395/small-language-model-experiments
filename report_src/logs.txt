Using device: mps

=== DELIVERABLE 1: TINY SHAKESPEARE ===
Loaded ../datasets/tiny_shakespeare
 - Vocab size: 65
 - Train tokens: 892117
 - Val tokens:   111669

>>> Running Sweep: Linear_Context_Sweep
   Testing block_size = 8
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.0841 | Val Loss 2.7660
Epoch 2: Train Loss 2.6290 | Val Loss 2.5567
Epoch 3: Train Loss 2.4967 | Val Loss 2.5169
Epoch 4: Train Loss 2.4512 | Val Loss 2.4453
Epoch 5: Train Loss 2.4116 | Val Loss 2.4995
   Testing block_size = 32
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.0986 | Val Loss 2.8325
Epoch 2: Train Loss 2.6957 | Val Loss 2.6887
Epoch 3: Train Loss 2.5552 | Val Loss 2.5975
Epoch 4: Train Loss 2.5192 | Val Loss 2.5530
Epoch 5: Train Loss 2.4514 | Val Loss 2.5678
   Testing block_size = 64
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.2250 | Val Loss 2.9960
Epoch 2: Train Loss 2.8613 | Val Loss 2.8905
Epoch 3: Train Loss 2.6848 | Val Loss 2.7325
Epoch 4: Train Loss 2.5897 | Val Loss 2.7301
Epoch 5: Train Loss 2.5605 | Val Loss 2.7076
   Testing block_size = 128
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.4448 | Val Loss 3.3436
Epoch 2: Train Loss 3.1646 | Val Loss 3.1196
Epoch 3: Train Loss 2.9644 | Val Loss 3.0106
Epoch 4: Train Loss 2.7955 | Val Loss 2.9084
Epoch 5: Train Loss 2.7433 | Val Loss 2.9128
   Testing block_size = 256
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 4.0350 | Val Loss 4.0056
Epoch 2: Train Loss 3.8212 | Val Loss 3.8623
Epoch 3: Train Loss 3.5687 | Val Loss 3.5944
Epoch 4: Train Loss 3.2671 | Val Loss 3.3838
Epoch 5: Train Loss 3.1441 | Val Loss 3.3534

>>> Running Sweep: MLP_Width_Sweep
   Testing hidden_size = 64
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.4107 | Val Loss 3.1482
Epoch 2: Train Loss 2.9807 | Val Loss 2.8567
Epoch 3: Train Loss 2.7835 | Val Loss 2.7164
Epoch 4: Train Loss 2.6846 | Val Loss 2.6930
Epoch 5: Train Loss 2.6507 | Val Loss 2.6630
   Testing hidden_size = 128
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.3059 | Val Loss 3.0736
Epoch 2: Train Loss 2.9462 | Val Loss 2.8340
Epoch 3: Train Loss 2.6998 | Val Loss 2.6583
Epoch 4: Train Loss 2.5839 | Val Loss 2.5904
Epoch 5: Train Loss 2.5698 | Val Loss 2.5811
   Testing hidden_size = 256
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.2666 | Val Loss 3.0510
Epoch 2: Train Loss 2.8537 | Val Loss 2.7600
Epoch 3: Train Loss 2.6206 | Val Loss 2.5628
Epoch 4: Train Loss 2.5500 | Val Loss 2.5028
Epoch 5: Train Loss 2.5044 | Val Loss 2.4824
   Testing hidden_size = 512
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.2238 | Val Loss 2.9595
Epoch 2: Train Loss 2.8594 | Val Loss 2.7427
Epoch 3: Train Loss 2.6493 | Val Loss 2.5865
Epoch 4: Train Loss 2.5188 | Val Loss 2.5233
Epoch 5: Train Loss 2.4680 | Val Loss 2.4934

>>> Running Sweep: MLP_Depth_Sweep
   Testing n_layers = 2
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.2515 | Val Loss 2.9819
Epoch 2: Train Loss 2.8500 | Val Loss 2.7438
Epoch 3: Train Loss 2.6314 | Val Loss 2.6161
Epoch 4: Train Loss 2.5714 | Val Loss 2.5604
Epoch 5: Train Loss 2.5502 | Val Loss 2.5649
   Testing n_layers = 4
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.3669 | Val Loss 3.1395                                            Epoch 2: Train Loss 2.9618 | Val Loss 2.8267                                            Epoch 3: Train Loss 2.7683 | Val Loss 2.7290
Epoch 4: Train Loss 2.6135 | Val Loss 2.6520
Epoch 5: Train Loss 2.5820 | Val Loss 2.6477
   Testing n_layers = 8
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.4136 | Val Loss 3.2100
Epoch 2: Train Loss 3.2182 | Val Loss 3.1821                                            Epoch 3: Train Loss 3.1501 | Val Loss 3.1624                                            Epoch 4: Train Loss 3.0980 | Val Loss 3.0975
Epoch 5: Train Loss 3.0652 | Val Loss 3.0856

>>> Running Sweep: AttentionOnly_Heads_Sweep
   Testing n_head = 2
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps
Epoch 1: Train Loss 3.2918 | Val Loss 2.8339
Epoch 2: Train Loss 2.7616 | Val Loss 2.6714
Epoch 3: Train Loss 2.6727 | Val Loss 2.6273
Epoch 4: Train Loss 2.6433 | Val Loss 2.6173
Epoch 5: Train Loss 2.6364 | Val Loss 2.6069
   Testing n_head = 4
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps
Epoch 1: Train Loss 3.3232 | Val Loss 2.8930
Epoch 2: Train Loss 2.7716 | Val Loss 2.6637
Epoch 3: Train Loss 2.6635 | Val Loss 2.6186
Epoch 4: Train Loss 2.6325 | Val Loss 2.6049
Epoch 5: Train Loss 2.6265 | Val Loss 2.5980
   Testing n_head = 8
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps                             Epoch 1: Train Loss 3.3224 | Val Loss 2.9035                                            Epoch 2: Train Loss 2.7897 | Val Loss 2.6729                                            Epoch 3: Train Loss 2.6670 | Val Loss 2.6242                                            Epoch 4: Train Loss 2.6381 | Val Loss 2.6041                                            Epoch 5: Train Loss 2.6295 | Val Loss 2.6009
>>> Running Sweep: Transformer_Heads_Sweep
   Testing n_head = 2
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps
Epoch 1: Train Loss 3.0219 | Val Loss 2.6424
Epoch 2: Train Loss 2.6088 | Val Loss 2.5414
Epoch 3: Train Loss 2.5373 | Val Loss 2.4882
Epoch 4: Train Loss 2.5084 | Val Loss 2.4720
Epoch 5: Train Loss 2.4988 | Val Loss 2.4750
   Testing n_head = 4
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps
Epoch 1: Train Loss 3.0399 | Val Loss 2.6490
Epoch 2: Train Loss 2.6115 | Val Loss 2.5513
Epoch 3: Train Loss 2.5439 | Val Loss 2.5049
Epoch 4: Train Loss 2.5175 | Val Loss 2.4923
Epoch 5: Train Loss 2.5027 | Val Loss 2.4902
   Testing n_head = 8
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps
Epoch 1: Train Loss 3.0623 | Val Loss 2.6686
Epoch 2: Train Loss 2.6185 | Val Loss 2.5549
Epoch 3: Train Loss 2.5446 | Val Loss 2.5190
Epoch 4: Train Loss 2.5200 | Val Loss 2.5035
Epoch 5: Train Loss 2.5099 | Val Loss 2.4967

>>> Running Sweep: Transformer_Depth_Sweep
   Testing n_layers = 1
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps
Epoch 1: Train Loss 3.4679 | Val Loss 3.0340
Epoch 2: Train Loss 2.9302 | Val Loss 2.7881
Epoch 3: Train Loss 2.7870 | Val Loss 2.7032
Epoch 4: Train Loss 2.7417 | Val Loss 2.6780
Epoch 5: Train Loss 2.7230 | Val Loss 2.6759
   Testing n_layers = 2
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps
Epoch 1: Train Loss 3.3792 | Val Loss 2.9942
Epoch 2: Train Loss 2.8717 | Val Loss 2.7165
Epoch 3: Train Loss 2.7272 | Val Loss 2.6460
Epoch 4: Train Loss 2.6807 | Val Loss 2.6244
Epoch 5: Train Loss 2.6692 | Val Loss 2.6228
   Testing n_layers = 4
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps
Epoch 1: Train Loss 3.2518 | Val Loss 2.8574
Epoch 2: Train Loss 2.7741 | Val Loss 2.6462
Epoch 3: Train Loss 2.6555 | Val Loss 2.5934
Epoch 4: Train Loss 2.6148 | Val Loss 2.5873
Epoch 5: Train Loss 2.6078 | Val Loss 2.5794
   Testing n_layers = 6
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps
Epoch 1: Train Loss 3.2298 | Val Loss 2.8184
Epoch 2: Train Loss 2.7331 | Val Loss 2.6226
Epoch 3: Train Loss 2.6245 | Val Loss 2.5818
Epoch 4: Train Loss 2.5827 | Val Loss 2.5586
Epoch 5: Train Loss 2.5739 | Val Loss 2.5576

>>> Running Sweep: Optimizer_Sweep
   Testing optimizer = adamw
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.1769 | Val Loss 2.7395
Epoch 2: Train Loss 2.6795 | Val Loss 2.5898
Epoch 3: Train Loss 2.5840 | Val Loss 2.5417
Epoch 4: Train Loss 2.5556 | Val Loss 2.5312
Epoch 5: Train Loss 2.5499 | Val Loss 2.5288
   Testing optimizer = sgd
--> Starting: Model | Opt: SGD | LR: 0.001 | Device: mps
Epoch 1: Train Loss 4.1492 | Val Loss 3.8837
Epoch 2: Train Loss 3.7306 | Val Loss 3.5303
Epoch 3: Train Loss 3.5137 | Val Loss 3.4164
Epoch 4: Train Loss 3.4375 | Val Loss 3.3793
Epoch 5: Train Loss 3.4190 | Val Loss 3.3695

>>> Running Sweep: LearningRate_Sweep
   Testing lr = 0.01
--> Starting: Model | Opt: ADAMW | LR: 0.01 | Device: mps
Epoch 1: Train Loss 2.6622 | Val Loss 2.4684
Epoch 2: Train Loss 2.4051 | Val Loss 2.2824
Epoch 3: Train Loss 2.2647 | Val Loss 2.1604
Epoch 4: Train Loss 2.1818 | Val Loss 2.1106
Epoch 5: Train Loss 2.1528 | Val Loss 2.0967
   Testing lr = 0.001
--> Starting: Model | Opt: ADAMW | LR: 0.001 | Device: mps
Epoch 1: Train Loss 3.1368 | Val Loss 2.7181
Epoch 2: Train Loss 2.6647 | Val Loss 2.5772
Epoch 3: Train Loss 2.5769 | Val Loss 2.5316
Epoch 4: Train Loss 2.5470 | Val Loss 2.5141
Epoch 5: Train Loss 2.5375 | Val Loss 2.5122
   Testing lr = 0.0005
--> Starting: Model | Opt: ADAMW | LR: 0.0005 | Device: mps
Epoch 1: Train Loss 3.3624 | Val Loss 2.9753
Epoch 2: Train Loss 2.8742 | Val Loss 2.7364
Epoch 3: Train Loss 2.7316 | Val Loss 2.6539
Epoch 4: Train Loss 2.6823 | Val Loss 2.6282
Epoch 5: Train Loss 2.6693 | Val Loss 2.6279
   Testing lr = 0.0001
--> Starting: Model | Opt: ADAMW | LR: 0.0001 | Device: mps
Epoch 1: Train Loss 3.8900 | Val Loss 3.4891
Epoch 2: Train Loss 3.3940 | Val Loss 3.2877
Epoch 3: Train Loss 3.2774 | Val Loss 3.2256
Epoch 4: Train Loss 3.2376 | Val Loss 3.1957
Epoch 5: Train Loss 3.2237 | Val Loss 3.1934

--- GENERATING DELIVERABLE 1 ARTIFACTS ---
Saved plot: ../report_src/d1_0_3_linear_context.png
Saved plot: ../report_src/d1_0_3_mlp_width.png
Saved plot: ../report_src/d1_0_3_tfm_heads.png
Saved plot: ../report_src/d1_0_4_flops_vs_ll.png
Saved generated text to: ../report_src/d1_0_5_hamlet_generation.txt

=== DELIVERABLE 2: WORD LEVEL (PTB & WIKITEXT) ===
Training Config for D2: {'block_size': 64, 'batch_size': 64, 'epochs': 10, 'n_embd': 64, 'n_head': 4, 'n_layers': 2, 'optimizer': 'adamw', 'dropout': 0.2, 'lr': 0.01, 'model_type': 'transformer'}

Training on PTB...
Loaded ../datasets/ptb
 - Vocab size: 9999
 - Train tokens: 887521
 - Val tokens:   70390
--> Starting: Model | Opt: ADAMW | LR: 0.01 | Device: mps
Epoch 1: Train Loss 6.7224 | Val Loss 6.3097
Epoch 2: Train Loss 6.0633 | Val Loss 5.8659
Epoch 3: Train Loss 5.7198 | Val Loss 5.6853
Epoch 4: Train Loss 5.5420 | Val Loss 5.5919
Epoch 5: Train Loss 5.3938 | Val Loss 5.5299
Epoch 6: Train Loss 5.2931 | Val Loss 5.4760
Epoch 7: Train Loss 5.2483 | Val Loss 5.4676
Epoch 8: Train Loss 5.2136 | Val Loss 5.4165
Epoch 9: Train Loss 5.1913 | Val Loss 5.4169
Epoch 10: Train Loss 5.1714 | Val Loss 5.4298
Saved plot: ../report_src/d2_ptb_0_2_flops_vs_ll.png
--- PTB Generated Sample ---
the school announced that the u.s. district approved for sale of american counterparts each period and legislative version of young and hungary the soviet union the paper bay area he wanted to begin declaring that death and mr. <unk> although management as insurance is increasingly enough for he notes were well nearly $ N a unit is seeking some bidder to take any financial planning reports to transfer and aliens says it has agreed to enters its role almost $ N billion will be developed rates ms. <unk> information let by business for a its <unk> microwave and politics also said bed and <unk>
Saved generated text to: ../report_src/d2_0_3_ptb_generation.txt

Training on WikiText...
Loaded ../datasets/wikitext-2
 - Vocab size: 33277
 - Train tokens: 2051910
 - Val tokens:   213886
--> Starting: Model | Opt: ADAMW | LR: 0.01 | Device: mps
Epoch 1: Train Loss 7.2660 | Val Loss 6.5814
Epoch 2: Train Loss 6.6350 | Val Loss 6.2912
Epoch 3: Train Loss 6.3594 | Val Loss 6.0694
Epoch 4: Train Loss 6.1350 | Val Loss 5.9369
Epoch 5: Train Loss 5.9758 | Val Loss 5.8234
Epoch 6: Train Loss 5.8525 | Val Loss 5.7650
Epoch 7: Train Loss 5.7676 | Val Loss 5.7228
Epoch 8: Train Loss 5.6924 | Val Loss 5.6584
Epoch 9: Train Loss 5.6661 | Val Loss 5.6518
Epoch 10: Train Loss 5.6478 | Val Loss 5.6559
Saved plot: ../report_src/d2_wiki_0_2_flops_vs_ll.png
--- WikiText Generated Sample ---
The history of machine learning begins the Canadian Union . The Guardian saw these of which the 2013 , three days Are the American tribe known . In the Japanese motion restaurant called these Nations strategic Meaning Mulder is toxic " The amber also allegorical v. notable " . In 2010 , the Marina called Frank replies he was prefaced that the lyrics found that play on the Gerard were frontman for 400 @,@ 000 @,@ 540 of the ceremony from Croatia were protected on solely for a constant release . That dispute and played that it were not spread to parallel directly , but also
Saved generated text to: ../report_src/d2_0_4_wiki_generation.txt

Done! All files for Deliverable 1 & 2 are in 'report_src/' folder.